{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Download a youtube video"
      ],
      "metadata": {
        "id": "tSh0U0s7C4Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQpbrqBlCuZz",
        "outputId": "1057025e-10b5-4c61-b8ec-a876a308736f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-12.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-12.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube"
      ],
      "metadata": {
        "id": "UfjJpdx9CzG7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "370PRFNnCUDa"
      },
      "outputs": [],
      "source": [
        "def Download(link):\n",
        "    youtubeObject = YouTube(link)\n",
        "    youtubeObject = youtubeObject.streams.get_highest_resolution()\n",
        "    try:\n",
        "        youtubeObject.download(output_path=\"\")\n",
        "    except:\n",
        "        print(\"An error has occurred\")\n",
        "    print(\"Download is completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "link = \"https://www.youtube.com/watch?v=HSPYgwP9R84\"\n",
        "Download(link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqCSHdAFCfJZ",
        "outputId": "36d3b78e-2094-47f1-85a9-f679c1e39e05"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download is completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object detection"
      ],
      "metadata": {
        "id": "A9bXnrZfDECJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "f8fqYlu9DIxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch.cuda\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as pltpatches\n",
        "from tqdm.notebook import tqdm\n",
        "import threading\n",
        "import time\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output, Image"
      ],
      "metadata": {
        "id": "YUtMaUvOC3JV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frame_to_tensor(frame: np.ndarray):\n",
        "    transform = transforms.ToTensor()\n",
        "    frame_t = transform(frame)\n",
        "    return frame_t"
      ],
      "metadata": {
        "id": "Ce2XENtDDF_L"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_frame_patches(frame: np.ndarray, patch_size):\n",
        "    \"\"\"\n",
        "    Function to split the frame into patches of size @patch_dim\n",
        "    :param frame: the frame of the video\n",
        "    :param patch_size: the dimension of the patches\n",
        "    :return: the patches\n",
        "    \"\"\"\n",
        "    frame_t = frame_to_tensor(frame)\n",
        "    # unfold the tensor along the 0-dimension to get the batch dimension\n",
        "    patches = frame_t.data.unfold(0, 3, 3)\n",
        "\n",
        "    # create vertical patches (in the height dimension)\n",
        "    patches = patches.unfold(1, patch_size, patch_size)\n",
        "\n",
        "    # create horizontal patches (in width dimension)\n",
        "    patches = patches.unfold(2, patch_size, patch_size)\n",
        "\n",
        "    print(f\"Shape of the patches = {patches.shape}\")\n",
        "    return patches"
      ],
      "metadata": {
        "id": "df0Rqy8VDVPG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_id=\"openai/clip-vit-base-patch32\"):\n",
        "    \"\"\"\n",
        "    Function to load the transformer model and the respective preprocessor\n",
        "    :param model_id: id of the model to load\n",
        "    :return: the processor and the model requested\n",
        "    \"\"\"\n",
        "    processor = CLIPProcessor.from_pretrained(model_id)\n",
        "    model = CLIPModel.from_pretrained(model_id)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    return model, processor, device"
      ],
      "metadata": {
        "id": "JthU2htSDYIG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(model, processor, device, prompt, patches, patch_size, window, stride):\n",
        "    \"\"\"\n",
        "    Function to run the model and get the similarity scores\n",
        "    :param model: the Visual Transformer to be run\n",
        "    :param processor: the processor associated to the Transformer to run\n",
        "    :param device: the hardware devoted to run the model\n",
        "    :param patches: the patches drawn from the frame\n",
        "    :param patch_size: the size of the patches\n",
        "    :param window: the amount of patches seen by the model\n",
        "    :return: scores associated to the big patches\n",
        "    \"\"\"\n",
        "    scores = torch.zeros(patches.shape[1], patches.shape[2])\n",
        "    runs = torch.ones(patches.shape[1], patches.shape[2])\n",
        "\n",
        "    for Y in range(0, patches.shape[1]-window+1, stride):\n",
        "        for X in range(0, patches.shape[2]-window+1, stride):\n",
        "            big_patch = torch.zeros(patch_size * window, patch_size * window, 3)\n",
        "            patch_batch = patches[0, Y:Y+window, X:X+window]\n",
        "            for y in range(window):\n",
        "                for x in range(window):\n",
        "                    big_patch[\n",
        "                    y * patch_size:(y + 1) * patch_size, x * patch_size:(x + 1) * patch_size, :\n",
        "                    ] = patch_batch[y, x].permute(1, 2, 0)\n",
        "            # we preprocess the image and class label with the CLIP processor\n",
        "            inputs = processor(\n",
        "                images=big_patch,  # big patch image sent to CLIP\n",
        "                return_tensors=\"pt\",  # tell CLIP to return pytorch tensor\n",
        "                text=prompt,  # class label sent to CLIP\n",
        "                padding=True\n",
        "            ).to(device) # move to device if possible\n",
        "\n",
        "            # calculate and retrieve similarity score\n",
        "            score = model(**inputs).logits_per_image.item()\n",
        "            # sum up similarity scores from current and previous big patches\n",
        "            # that were calculated for patches within the current window\n",
        "            scores[Y:Y+window, X:X+window] += score\n",
        "            # calculate the number of runs on each patch within the current window\n",
        "            runs[Y:Y+window, X:X+window] += 1\n",
        "    # calculate average scores\n",
        "    scores /= runs\n",
        "    # clip scores\n",
        "    for _ in range(3):\n",
        "        scores = np.clip(scores-scores.mean(), 0, np.inf)\n",
        "    # normalize scores\n",
        "    scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "    return scores"
      ],
      "metadata": {
        "id": "h5qcJ63SDajB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_box(scores, patch_size, threshold):\n",
        "    detection = scores > threshold\n",
        "    # find box corners\n",
        "    y_min, y_max = np.nonzero(detection)[:, 0].min().item(), np.nonzero(detection)[:, 0].max().item()+1\n",
        "    x_min, x_max = np.nonzero(detection)[:, 1].min().item(), np.nonzero(detection)[:, 1].max().item()+1\n",
        "    # convert from patch co-ords to pixel co-ords\n",
        "    y_min *= patch_size\n",
        "    y_max *= patch_size\n",
        "    x_min *= patch_size\n",
        "    x_max *= patch_size\n",
        "    # calculate box height and width\n",
        "    height = y_max - y_min\n",
        "    width = x_max - x_min\n",
        "    return x_min, y_min, width, height"
      ],
      "metadata": {
        "id": "gb4GX1XIDdwQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect(model, processor, device, prompts, frame, patch_size=64, window=3, stride=1, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Function to the detect the objects in the frame. It uses the frames to look for the specified items.\n",
        "    It creates a plot of the image containing the detected objects.\n",
        "    :param model: model to run for the inference\n",
        "    :param processor: processor associated to the model\n",
        "    :param device: the hardware used to run the inference\n",
        "    :param prompts: the objects to find in the frame\n",
        "    :param frame: the specified frame\n",
        "    :param patch_size: the size of the patches\n",
        "    :param window: the amount of patches to search in simultaneously\n",
        "    :return: the bounding box parameters\n",
        "    \"\"\"\n",
        "    colors = ['#FAFF00', '#8CF1FF']\n",
        "    # build image patches for detection\n",
        "    frame_patches = get_frame_patches(frame, patch_size)\n",
        "    frame_t = frame_to_tensor(frame)\n",
        "    # convert image to format for displaying with matplotlib\n",
        "    \"\"\"\n",
        "    image = np.moveaxis(frame_t.data.numpy(), 0, -1)\n",
        "    X = frame_patches.shape[1]\n",
        "    Y = frame_patches.shape[2]\n",
        "    # initialize plot to display image + bounding boxes\n",
        "    fig, ax = plt.subplots(figsize=(Y*0.5, X*0.5))\n",
        "    ax.imshow(image)\n",
        "    \"\"\"\n",
        "    bounding_box_list = []\n",
        "    # process image through object detection steps\n",
        "    for i, prompt in enumerate(tqdm(prompts)):\n",
        "        scores = run_inference(model, processor, device, prompt, frame_patches, patch_size, window, stride)\n",
        "        x, y, width, height = get_box(scores, patch_size, threshold)\n",
        "        if width > 0 and height > 0:\n",
        "          bounding_box_list.append((x, y, width, height))\n",
        "        # create the bounding box\n",
        "        # rect = pltpatches.Rectangle((x, y), width, height, linewidth=3, edgecolor=colors[i], facecolor='none')\n",
        "        # cv2.rectangle(frame, (x, y), (x+width, y+height), [0, 255, 0])\n",
        "        # add the patch to the Axes\n",
        "        # ax.add_patch(rect)\n",
        "    # cv2.imshow(\"Frame\", frame)\n",
        "    return bounding_box_list"
      ],
      "metadata": {
        "id": "XPlZxSm3Df6Z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Online Object Detection"
      ],
      "metadata": {
        "id": "52wVwhfTMCKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video_and_detect(input_file_path, prompts):\n",
        "    \"\"\"\n",
        "    Function to show the video in an external window.\n",
        "    When the video is paused the detection algorithm is run with the specified prompts.\n",
        "    @param: input_file_path path of the video to be shown\n",
        "    \"\"\"\n",
        "    # Show the video\n",
        "    capture = cv2.VideoCapture(input_file_path)\n",
        "    frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    fps = np.ceil(capture.get(cv2.CAP_PROP_FPS))\n",
        "    print(f\"fps:{fps:.2f}, frame width: {frame_width}, frame height: {frame_height}\")\n",
        "\n",
        "    model, processor, device = load_model()\n",
        "\n",
        "    while capture.isOpened():\n",
        "        ret, frame = capture.read()\n",
        "\n",
        "        if ret:\n",
        "            clear_output(wait=True)\n",
        "            cv2_imshow(frame)\n",
        "            # Press Q on keyboard to exit\n",
        "            key = cv2.waitKey(25)\n",
        "            if key & 0xFF == ord('q'):\n",
        "                break\n",
        "            elif key == 32:\n",
        "                t0 = time.time()\n",
        "                detect(model, processor, prompts=prompts, device=device, frame=frame)\n",
        "                t1 = time.time()\n",
        "                print(f\"Time for detection = {t1-t0}\")\n",
        "                cv2.waitKey()\n",
        "        # Break the loop\n",
        "        else:\n",
        "            break\n",
        "    # When everything done, release\n",
        "    # the video capture object\n",
        "    capture.release()\n",
        "\n",
        "    # Closes all the frames\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "0DaX2CljDjCP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_video_and_detect(\"/content/The Devil Wears Prada (45) Movie CLIP - Andy Gets a Makeover (2006) HD.mp4\", prompts=[\"black t-shirt\"])"
      ],
      "metadata": {
        "id": "QAoys8GaDn-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Offline object detection\n"
      ],
      "metadata": {
        "id": "3FoM4XYOL4x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pafy"
      ],
      "metadata": {
        "id": "-thgmO98L8_k",
        "outputId": "ba36c1ad-1fe7-46ca-d4c6-b1b8a8bba055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pafy\n",
            "  Downloading pafy-0.5.5-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: pafy\n",
            "Successfully installed pafy-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube_dl==2020.12.7"
      ],
      "metadata": {
        "id": "BkfA5iTpMQZV",
        "outputId": "42d6e3e8-1c48-4d85-e16d-93e8a30fde7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube_dl==2020.12.7\n",
            "  Downloading youtube_dl-2020.12.7-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_dl\n",
            "Successfully installed youtube_dl-2020.12.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pafy\n",
        "import random\n",
        "import youtube_dl\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "o4oo-EHZMV27"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the videon \n",
        "ydl_opts = {}\n",
        "ydl=youtube_dl.YoutubeDL(ydl_opts)\n",
        "info_dict=ydl.extract_info(\"https://www.youtube.com/watch?v=HSPYgwP9R84\", download=False)\n",
        "formats = info_dict.get('formats', None)\n",
        "for f_number, f in enumerate(formats):\n",
        "  if f.get('format_note', None) == '360p' and f.get('ext', None) == 'mp4' and f.get('filesize', None) != None:\n",
        "    print(f)"
      ],
      "metadata": {
        "id": "_c-M_WGVf7Yk",
        "outputId": "b1f456ce-5924-4171-dbf2-0bb8f578787c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] HSPYgwP9R84: Downloading webpage\n",
            "{'format_id': '396', 'url': 'https://rr2---sn-5hne6nzk.googlevideo.com/videoplayback?expire=1679949538&ei=gqohZJvnCpDM1gLDzKqoCA&ip=35.234.175.150&id=o-ANCpNXcBtqoaF_3scD28GWFgLXuX75uWBogwkk31ZbZO&itag=396&aitags=133%2C134%2C135%2C136%2C137%2C160%2C242%2C243%2C244%2C247%2C248%2C278%2C394%2C395%2C396%2C397%2C398%2C399&source=youtube&requiressl=yes&mh=6q&mm=31%2C26&mn=sn-5hne6nzk%2Csn-5goeenes&ms=au%2Conr&mv=m&mvi=2&pl=20&vprv=1&mime=video%2Fmp4&ns=MzaF3RbycjJCPicdbmsayDUM&gir=yes&clen=3911216&dur=159.534&lmt=1617694467598556&mt=1679927595&fvip=1&keepalive=yes&fexp=24007246&c=WEB&txp=5531432&n=VRY32ZrMH7RDwHineF&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cvprv%2Cmime%2Cns%2Cgir%2Cclen%2Cdur%2Clmt&sig=AOq0QJ8wRQIhALL_lr8tfMZU8uuoSBJ-lHvpA1IaIIoL-bToPawfiPqWAiAzPS--n5ew4EcA0JE5zlDtjglKBcvy8dznhhf1MMbOlw%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl&lsig=AG3C_xAwRAIgar02HsWVxO92XCqhVlyXD6KDJZqqXUOuH4XDMY2hmcwCIBrJ1C13bAEn1xEx5ghve4HMnWusEtUNy_sioCvgPQxk&ratebypass=yes', 'player_url': None, 'acodec': 'none', 'vcodec': 'av01.0.01M.08', 'asr': None, 'filesize': 3911216, 'format_note': '360p', 'fps': 24, 'height': 360, 'tbr': 293.189, 'width': 640, 'ext': 'mp4', 'downloader_options': {'http_chunk_size': 10485760}, 'format': '396 - 640x360 (360p)', 'protocol': 'https', 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3545.3 Safari/537.36', 'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'en-us,en;q=0.5'}}\n",
            "{'format_id': '134', 'url': 'https://rr2---sn-5hne6nzk.googlevideo.com/videoplayback?expire=1679949538&ei=gqohZJvnCpDM1gLDzKqoCA&ip=35.234.175.150&id=o-ANCpNXcBtqoaF_3scD28GWFgLXuX75uWBogwkk31ZbZO&itag=134&aitags=133%2C134%2C135%2C136%2C137%2C160%2C242%2C243%2C244%2C247%2C248%2C278%2C394%2C395%2C396%2C397%2C398%2C399&source=youtube&requiressl=yes&mh=6q&mm=31%2C26&mn=sn-5hne6nzk%2Csn-5goeenes&ms=au%2Conr&mv=m&mvi=2&pl=20&vprv=1&mime=video%2Fmp4&ns=MzaF3RbycjJCPicdbmsayDUM&gir=yes&clen=4020400&dur=159.534&lmt=1512882658926150&mt=1679927595&fvip=1&keepalive=yes&fexp=24007246&c=WEB&n=VRY32ZrMH7RDwHineF&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cvprv%2Cmime%2Cns%2Cgir%2Cclen%2Cdur%2Clmt&sig=AOq0QJ8wRgIhAKN4pPi6D8-RKTYaVbUK3HYX-hRGQdqNXEK3sm_rxuQ2AiEAyre84duv5mODS5DshcWSNXnpLBDnnLdb-m1pg4uW-Gg%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl&lsig=AG3C_xAwRAIgar02HsWVxO92XCqhVlyXD6KDJZqqXUOuH4XDMY2hmcwCIBrJ1C13bAEn1xEx5ghve4HMnWusEtUNy_sioCvgPQxk&ratebypass=yes', 'player_url': None, 'ext': 'mp4', 'height': 360, 'format_note': '360p', 'vcodec': 'avc1.4d401e', 'asr': None, 'filesize': 4020400, 'fps': 24, 'tbr': 346.202, 'width': 640, 'acodec': 'none', 'downloader_options': {'http_chunk_size': 10485760}, 'format': '134 - 640x360 (360p)', 'protocol': 'https', 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3545.3 Safari/537.36', 'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'en-us,en;q=0.5'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(video_url, prompts, frame_per_second=5, patch_size=64, window=3, stride=1, threshold=0.5):\n",
        "  \"\"\"\n",
        "  Function to detect the specified object in each frame of the video\n",
        "  @param: video_url is the link to the video to analyse\n",
        "  @param: prompts is a list of texts descriving the objects to detect\n",
        "  @param: frame_per_second is the number of frame to analyse in a second\n",
        "  @param: the others are related to the object detection\n",
        "  \"\"\"\n",
        "  # Get the video\n",
        "  ydl_opts = {}\n",
        "  ydl=youtube_dl.YoutubeDL(ydl_opts)\n",
        "  info_dict=ydl.extract_info(video_url, download=False)\n",
        "\n",
        "  # Instanciate the lists\n",
        "  images_list = []\n",
        "  filenames_list = []\n",
        "  bounding_box_list = []\n",
        "\n",
        "  model, processor, device = load_model()\n",
        "\n",
        "  formats = info_dict.get('formats', None)\n",
        "  print(\"Obtaining frames\")\n",
        "  for f_number, f in enumerate(formats):\n",
        "\n",
        "    # If the resolution is 360p and if the format is webm (faster than mp4)\n",
        "    if f.get('format_note', None) == '360p' and f.get('ext', None) == 'webm' and f.get('filesize', None) != None:\n",
        "      # Get the url\n",
        "      url = f.get('url', None)\n",
        "\n",
        "      # Define how many frames to skip between each analysis\n",
        "      skip_frames = int(f['fps'] / frame_per_second)\n",
        "\n",
        "      cap = cv.VideoCapture(url)\n",
        "      current_frame = 0\n",
        "      t0 = time.time()\n",
        "      # Till the end of the video\n",
        "      while True:\n",
        "        print(f'Computing frame number {len(images_list)}')\n",
        "        # Get the frame\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Define a name for the frame\n",
        "        filename = r\"video_shot\" + str(len(images_list)) + \".png\"\n",
        "        # If save the frame\n",
        "        # cv.imwrite(filename.format(count), frame)\n",
        "        # Append the frame and the filename in the lists\n",
        "        images_list.append(frame)\n",
        "        filenames_list.append(filename)\n",
        "        bounding_boxes = detect(model,\n",
        "                                processor, \n",
        "                                prompts=prompts, \n",
        "                                device=device, \n",
        "                                frame=frame,\n",
        "                                patch_size=patch_size, \n",
        "                                window=window, \n",
        "                                stride=stride, \n",
        "                                threshold=threshold)\n",
        "        bounding_box_list.append(bounding_boxes)\n",
        "\n",
        "        # Skip some frames\n",
        "        current_frame += skip_frames\n",
        "        cap.set(1, current_frame)\n",
        "        if cv.waitKey(30) & 0xFF == ord('q'):\n",
        "            break\n",
        "      print(\"Saved {} images with format {} and resolution {} in {:.4} seconds ({:.4} minutes)\".format(len(images_list), f.get('ext', None), f.get('format_note',None), (time.time() - t0), (time.time() - t0) / 60 ))\n",
        "      # If a valid format has been found and analysed\n",
        "      # if len(images_list) > 0:\n",
        "      #  break\n",
        "      # cap.release()\n",
        "  # Return the lists\n",
        "  return  images_list, filenames_list, bounding_box_list"
      ],
      "metadata": {
        "id": "4nVJr78DLJ-l"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_frames(images, row_dim=4, num_of_images=None):\n",
        "  \"\"\"\n",
        "  Function to show all the frames in the list\n",
        "  @params: images is the list of images\n",
        "  @params: row_dim is the number of images to show in a row\n",
        "  @param: num_of_images indicates how many images to show\n",
        "  \"\"\"\n",
        "  if num_of_images == None or num_of_images <= 0 or num_of_images > len(images):\n",
        "    num_of_images = len(images)\n",
        "\n",
        "  fig, axs = plt.subplots(int(num_of_images / row_dim) + 1, row_dim, figsize=(20, len(images[0][0]) // row_dim))\n",
        "\n",
        "  for i in tqdm(range(num_of_images)):\n",
        "    axs[int(i/row_dim), i%row_dim].imshow(images[i])\n",
        "    axs[int(i/row_dim), i%row_dim].set_title('Frame: {}'.format(i))\n",
        "\n",
        "  plt.plot()"
      ],
      "metadata": {
        "id": "n9l0IiIUDyCT"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_frames_with_bounding_box(images_list, bounding_boxes, row_dim=6, num_of_images=None):\n",
        "  \"\"\"\n",
        "  Function to show all the frames in the list with the found bounding box\n",
        "  @params: images_list is the list of images\n",
        "  @params: bounding_boxes is a list of bounding box associated with the images.\n",
        "            For each image there a list of tuple: (x, y, width, heigth)\n",
        "  @params: row_dim is the number of images to show in a row\n",
        "  @param: num_of_images indicates how many images to show\n",
        "  \"\"\"\n",
        "  # Create a copy of the list\n",
        "  images = images_list.copy()\n",
        "\n",
        "  if num_of_images == None or num_of_images <= 0 or num_of_images > len(images):\n",
        "    num_of_images = len(images)\n",
        "\n",
        "  fig, axs = plt.subplots(int(num_of_images / row_dim) + 1, row_dim, figsize=(20, len(images[0][0]) // row_dim))\n",
        "\n",
        "  for i in tqdm(range(num_of_images)):\n",
        "    # If there is at least one box\n",
        "    if len(bounding_boxes[i]) > 0:\n",
        "      for bb in bounding_boxes[i]:\n",
        "        cv2.rectangle(images[i], # frame\n",
        "                      (bb[0], # x\n",
        "                       bb[1]),# y \n",
        "                      (bb[0]+bb[2], # width\n",
        "                       bb[1]+bb[3]),# length \n",
        "                      [random.randint(128, 255), random.randint(128, 255), random.randint(128, 255)]) # random color\n",
        "    axs[int(i/row_dim), i%row_dim].imshow(images[i])\n",
        "    axs[int(i/row_dim), i%row_dim].set_title('Frame: {}'.format(i))\n",
        "\n",
        "  plt.plot()"
      ],
      "metadata": {
        "id": "RASBg4raPnO_"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_list, image_names_list, bounding_box_list = detect_objects(\n",
        "    video_url=\"https://www.youtube.com/watch?v=HSPYgwP9R84\",\n",
        "    prompts=[\"a small lamp made of white glass\", \"black single-breasted jacket\"], \n",
        "    frame_per_second=1, \n",
        "    patch_size=64, \n",
        "    window=4, \n",
        "    stride=1, \n",
        "    threshold=0.5)"
      ],
      "metadata": {
        "id": "0Q0Etpg9Qv3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The number of frames is {len(images_list)}')"
      ],
      "metadata": {
        "id": "nrytpoPHTpre",
        "outputId": "599b5346-d663-4acf-a63b-e72a7c775962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of frames is 160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_frames(images=images_list, row_dim=4, num_of_images=None)"
      ],
      "metadata": {
        "id": "ADq8s5CwSJO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_frames_with_bounding_box(images_list=images_list, bounding_boxes=bounding_box_list, row_dim=4, num_of_images=None)"
      ],
      "metadata": {
        "id": "m7OE-7K4Xkt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZMRFdn1YQRG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}