{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSh0U0s7C4Wm"
      },
      "source": [
        "## Download a youtube video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QQpbrqBlCuZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d212eeb5-1aca-41e9-da42-53a1c404e55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-12.1.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-12.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UfjJpdx9CzG7"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "370PRFNnCUDa"
      },
      "outputs": [],
      "source": [
        "def Download(link):\n",
        "    youtubeObject = YouTube(link)\n",
        "    youtubeObject = youtubeObject.streams.get_highest_resolution()\n",
        "    try:\n",
        "        youtubeObject.download(output_path=\"\")\n",
        "    except:\n",
        "        print(\"An error has occurred\")\n",
        "    print(\"Download is completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tqCSHdAFCfJZ"
      },
      "outputs": [],
      "source": [
        "link = \"https://www.youtube.com/watch?v=HSPYgwP9R84\"\n",
        "# Download(link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9bXnrZfDECJ"
      },
      "source": [
        "## Object detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f8fqYlu9DIxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9238762a-0e44-4793-fe15-6be3053583a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YUtMaUvOC3JV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch.cuda\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as pltpatches\n",
        "from tqdm.notebook import tqdm\n",
        "import threading\n",
        "import time\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output, Image\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ce2XENtDDF_L"
      },
      "outputs": [],
      "source": [
        "def frame_to_tensor(frame: np.ndarray):\n",
        "    transform = transforms.ToTensor()\n",
        "    frame_t = transform(frame)\n",
        "    return frame_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "df0Rqy8VDVPG"
      },
      "outputs": [],
      "source": [
        "def get_frame_patches(frame: np.ndarray, patch_size):\n",
        "    \"\"\"\n",
        "    Function to split the frame into patches of size @patch_dim\n",
        "    :param frame: the frame of the video\n",
        "    :param patch_size: the dimension of the patches\n",
        "    :return: the patches\n",
        "    \"\"\"\n",
        "    frame_t = frame_to_tensor(frame)\n",
        "    # unfold the tensor along the 0-dimension to get the batch dimension\n",
        "    patches = frame_t.data.unfold(0, 3, 3)\n",
        "\n",
        "    # create vertical patches (in the height dimension)\n",
        "    patches = patches.unfold(1, patch_size, patch_size)\n",
        "\n",
        "    # create horizontal patches (in width dimension)\n",
        "    patches = patches.unfold(2, patch_size, patch_size)\n",
        "\n",
        "    print(f\"Shape of the patches = {patches.shape}\")\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JthU2htSDYIG"
      },
      "outputs": [],
      "source": [
        "def load_model(model_id=\"openai/clip-vit-base-patch32\"):\n",
        "    \"\"\"\n",
        "    Function to load the transformer model and the respective preprocessor\n",
        "    :param model_id: id of the model to load\n",
        "    :return: the processor and the model requested\n",
        "    \"\"\"\n",
        "    processor = CLIPProcessor.from_pretrained(model_id)\n",
        "    model = CLIPModel.from_pretrained(model_id)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    return model, processor, device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h5qcJ63SDajB"
      },
      "outputs": [],
      "source": [
        "def run_inference(model, processor, device, prompt, patches, patch_size, window, stride):\n",
        "    \"\"\"\n",
        "    Function to run the model and get the similarity scores\n",
        "    :param model: the Visual Transformer to be run\n",
        "    :param processor: the processor associated to the Transformer to run\n",
        "    :param device: the hardware devoted to run the model\n",
        "    :param patches: the patches drawn from the frame\n",
        "    :param patch_size: the size of the patches\n",
        "    :param window: the amount of patches seen by the model\n",
        "    :return: scores associated to the big patches\n",
        "    \"\"\"\n",
        "    scores = torch.zeros(patches.shape[1], patches.shape[2])\n",
        "    runs = torch.ones(patches.shape[1], patches.shape[2])\n",
        "\n",
        "    for Y in range(0, patches.shape[1]-window+1, stride):\n",
        "        for X in range(0, patches.shape[2]-window+1, stride):\n",
        "            big_patch = torch.zeros(patch_size * window, patch_size * window, 3)\n",
        "            patch_batch = patches[0, Y:Y+window, X:X+window]\n",
        "            for y in range(window):\n",
        "                for x in range(window):\n",
        "                    big_patch[\n",
        "                    y * patch_size:(y + 1) * patch_size, x * patch_size:(x + 1) * patch_size, :\n",
        "                    ] = patch_batch[y, x].permute(1, 2, 0)\n",
        "            # we preprocess the image and class label with the CLIP processor\n",
        "            inputs = processor(\n",
        "                images=big_patch,  # big patch image sent to CLIP\n",
        "                return_tensors=\"pt\",  # tell CLIP to return pytorch tensor\n",
        "                text=prompt,  # class label sent to CLIP\n",
        "                padding=True\n",
        "            ).to(device) # move to device if possible\n",
        "\n",
        "            # calculate and retrieve similarity score\n",
        "            score = model(**inputs).logits_per_image.item()\n",
        "            # sum up similarity scores from current and previous big patches\n",
        "            # that were calculated for patches within the current window\n",
        "            scores[Y:Y+window, X:X+window] += score\n",
        "            # calculate the number of runs on each patch within the current window\n",
        "            runs[Y:Y+window, X:X+window] += 1\n",
        "    # calculate average scores\n",
        "    scores /= runs\n",
        "    # clip scores\n",
        "    for _ in range(3):\n",
        "        scores = np.clip(scores-scores.mean(), 0, np.inf)\n",
        "    # normalize scores\n",
        "    scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "    #scores = (scores - scores.mean()) / scores.std()\n",
        "    #print(f'The max score is {scores.max()}, while the min score is {scores.min()}')\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JZY8lZoQroNW"
      },
      "outputs": [],
      "source": [
        "def get_box(scores, patch_size, threshold):\n",
        "    detection = scores > threshold\n",
        "    # find box corners\n",
        "    y_min, y_max = np.nonzero(detection)[:, 0].min().item(), np.nonzero(detection)[:, 0].max().item()+1\n",
        "    x_min, x_max = np.nonzero(detection)[:, 1].min().item(), np.nonzero(detection)[:, 1].max().item()+1\n",
        "    print(f'The x_min is {x_min} and x_max is {x_max} and their scores are {(np.nonzero(detection)[:, 1].min()), np.nonzero(detection)[:, 1].max()}')\n",
        "    print(f'The detection for the y is {np.nonzero(detection)[:, 0]}')\n",
        "    print(f'The detection for the x is {np.nonzero(detection)[:, 1]}')\n",
        "    # convert from patch co-ords to pixel co-ords\n",
        "    y_min *= patch_size\n",
        "    y_max *= patch_size\n",
        "    x_min *= patch_size\n",
        "    x_max *= patch_size\n",
        "    # calculate box height and width\n",
        "    height = y_max - y_min\n",
        "    width = x_max - x_min\n",
        "    return x_min, y_min, width, height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gb4GX1XIDdwQ"
      },
      "outputs": [],
      "source": [
        "def get_multi_box(scores, patch_size, threshold):\n",
        "\n",
        "    # Convert the scores to a numpy array if it's a tensor\n",
        "    if torch.is_tensor(input):\n",
        "      scores = scores.numpy()\n",
        "    # print(f\"The scores' shape si {scores.shape}\")\n",
        "\n",
        "    # Print the scores as an image to see the locations\n",
        "    '''\n",
        "    print(f'Printing the scores matrix')\n",
        "    row_string = ''\n",
        "    for row in scores:\n",
        "      for col in row:\n",
        "        row_string += '{:2f} '.format(col)\n",
        "      row_string += '\\n'\n",
        "    print(row_string)\n",
        "    '''\n",
        "\n",
        "    # Define the matrix with only the good scores\n",
        "    detection = np.zeros((scores.shape))\n",
        "\n",
        "    # Consider only the region of interest in the image\n",
        "    detection[scores > threshold] = scores[scores > threshold]\n",
        "    # detection = np.nonzero(detection)\n",
        "    print(f'The shape of the detection is {detection.shape}')\n",
        "\n",
        "    # Print the scores as an image to see the locations\n",
        "    '''\n",
        "    print(f'Printing the detection matrix')\n",
        "    row_string = ''\n",
        "    for row in detection:\n",
        "      for col in row:\n",
        "        row_string += '{:2f} '.format(col)\n",
        "      row_string += '\\n'\n",
        "    print(row_string)\n",
        "    '''\n",
        "\n",
        "    bb_dict = {}\n",
        "    finished = False\n",
        "    bb_finished = False\n",
        "    bb_idx = 0\n",
        "    x_coo = 0\n",
        "    y_coo = 0\n",
        "    elem_to_check = []\n",
        "    num_cells_different_from_zero = 0\n",
        "\n",
        "    while not finished:\n",
        "      # Init the dictionary\n",
        "      bb_dict[bb_idx] = {}\n",
        "      bb_dict[bb_idx]['x'] = []\n",
        "      bb_dict[bb_idx]['y'] = []\n",
        "      # Add the current element to the check list\n",
        "      elem_to_check.append((x_coo, y_coo))\n",
        "      # Till there are elements to check\n",
        "      while len(elem_to_check) > 0:\n",
        "        current_position = elem_to_check.pop(0)\n",
        "        # If the current location is different from zero\n",
        "        # print(f'Current cells is {current_position}')\n",
        "        if detection[current_position[0], current_position[1]] != 0:\n",
        "          num_cells_different_from_zero += 1\n",
        "          # Add the current elem to the dict\n",
        "          bb_dict[bb_idx]['x'].append(current_position[0])\n",
        "          bb_dict[bb_idx]['y'].append(current_position[1])\n",
        "          # Set to zero the location already added\n",
        "          detection[current_position[0], current_position[1]] = 0\n",
        "          # Check the 4 cells around if they are still valid cells\n",
        "          if current_position[1]+1 < detection.shape[1]:\n",
        "            elem_to_check.append((current_position[0], current_position[1]+1))\n",
        "          if current_position[0]+1 < detection.shape[0]:\n",
        "            elem_to_check.append((current_position[0]+1, current_position[1]))\n",
        "          if current_position[0]-1 >= 0:\n",
        "            elem_to_check.append((current_position[0]-1, current_position[1]))\n",
        "          if current_position[1]-1 >= 0:\n",
        "            elem_to_check.append((current_position[0], current_position[1]-1))\n",
        "            \n",
        "      # Go to the next bb\n",
        "      bb_idx += 1\n",
        "      # Update x_coo and y_coo\n",
        "      if x_coo < detection.shape[0] - 1:\n",
        "        x_coo += 1\n",
        "      elif y_coo < detection.shape[1] - 1:\n",
        "        x_coo = 0\n",
        "        y_coo += 1\n",
        "      else: \n",
        "        # If in the last position\n",
        "        finished = True\n",
        "    print(f'Finished the computation of the bounding boxes')\n",
        "\n",
        "    final_list_of_bb = []\n",
        "    for key, item in bb_dict.items():\n",
        "      x_values = item['x']\n",
        "      y_values = item['y']\n",
        "\n",
        "      if len(x_values) > 0 and len(y_values) > 0:\n",
        "\n",
        "        '''\n",
        "        print(f'The points in the bounding box {key} are: ')\n",
        "        print(f'x: {x_values}')\n",
        "        print(f'y: {y_values}')\n",
        "        '''\n",
        "\n",
        "        # find box corners\n",
        "        y_min, y_max = min(y_values), max(y_values)+1\n",
        "        x_min, x_max = min(x_values), max(x_values)+1\n",
        "\n",
        "        # convert from patch co-ords to pixel co-ords\n",
        "        y_min *= patch_size\n",
        "        y_max *= patch_size\n",
        "        x_min *= patch_size\n",
        "        x_max *= patch_size\n",
        "\n",
        "        # calculate box height and width\n",
        "        width = y_max - y_min\n",
        "        height = x_max - x_min\n",
        "\n",
        "        # Append the bb\n",
        "        final_list_of_bb.append((y_min, x_min, width, height))\n",
        "\n",
        "    # Return the whole list\n",
        "    print(f'The number of boxes detected is {len(final_list_of_bb)}')\n",
        "    print(f'The number of cells different from zero is {num_cells_different_from_zero}')\n",
        "    return final_list_of_bb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XPlZxSm3Df6Z"
      },
      "outputs": [],
      "source": [
        "def detect(model, processor, device, prompts, frame, patch_size=64, window=3, stride=1, threshold=0.5, single_box=True):\n",
        "    \"\"\"\n",
        "    Function to the detect the objects in the frame. It uses the frames to look for the specified items.\n",
        "    It creates a plot of the image containing the detected objects.\n",
        "    :param model: model to run for the inference\n",
        "    :param processor: processor associated to the model\n",
        "    :param device: the hardware used to run the inference\n",
        "    :param prompts: the objects to find in the frame\n",
        "    :param frame: the specified frame\n",
        "    :param patch_size: the size of the patches\n",
        "    :param window: the amount of patches to search in simultaneously\n",
        "    :return: the bounding box parameters\n",
        "    \"\"\"\n",
        "    colors = ['#FAFF00', '#8CF1FF']\n",
        "    # build image patches for detection\n",
        "    frame_patches = get_frame_patches(frame, patch_size)\n",
        "    frame_t = frame_to_tensor(frame)\n",
        "    # convert image to format for displaying with matplotlib\n",
        "    \"\"\"\n",
        "    image = np.moveaxis(frame_t.data.numpy(), 0, -1)\n",
        "    X = frame_patches.shape[1]\n",
        "    Y = frame_patches.shape[2]\n",
        "    # initialize plot to display image + bounding boxes\n",
        "    fig, ax = plt.subplots(figsize=(Y*0.5, X*0.5))\n",
        "    ax.imshow(image)\n",
        "    \"\"\"\n",
        "    bounding_box_list = []\n",
        "    # process image through object detection steps\n",
        "    for i, prompt in enumerate(tqdm(prompts)):\n",
        "        scores = run_inference(model, processor, device, prompt, frame_patches, patch_size, window, stride)\n",
        "        if single_box:\n",
        "          x, y, width, height = get_box(scores, patch_size, threshold)\n",
        "          if width > 0 and height > 0:\n",
        "            bounding_box_list.append((x, y, width, height))\n",
        "        else: # If multi box\n",
        "          bb_list = get_multi_box(scores, patch_size, threshold)\n",
        "          for bb in bb_list:\n",
        "            bounding_box_list.append(bb)\n",
        "        # create the bounding box\n",
        "        # rect = pltpatches.Rectangle((x, y), width, height, linewidth=3, edgecolor=colors[i], facecolor='none')\n",
        "        # cv2.rectangle(frame, (x, y), (x+width, y+height), [0, 255, 0])\n",
        "        # add the patch to the Axes\n",
        "        # ax.add_patch(rect)\n",
        "    # cv2.imshow(\"Frame\", frame)\n",
        "    return bounding_box_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52wVwhfTMCKO"
      },
      "source": [
        "## Online Object Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0DaX2CljDjCP"
      },
      "outputs": [],
      "source": [
        "def show_video_and_detect(input_file_path, prompts):\n",
        "    \"\"\"\n",
        "    Function to show the video in an external window.\n",
        "    When the video is paused the detection algorithm is run with the specified prompts.\n",
        "    @param: input_file_path path of the video to be shown\n",
        "    \"\"\"\n",
        "    # Show the video\n",
        "    capture = cv2.VideoCapture(input_file_path)\n",
        "    frame_width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "    frame_height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "    fps = np.ceil(capture.get(cv2.CAP_PROP_FPS))\n",
        "    print(f\"fps:{fps:.2f}, frame width: {frame_width}, frame height: {frame_height}\")\n",
        "\n",
        "    model, processor, device = load_model()\n",
        "\n",
        "    while capture.isOpened():\n",
        "        ret, frame = capture.read()\n",
        "\n",
        "        if ret:\n",
        "            clear_output(wait=True)\n",
        "            cv2_imshow(frame)\n",
        "            # Press Q on keyboard to exit\n",
        "            key = cv2.waitKey(25)\n",
        "            if key & 0xFF == ord('q'):\n",
        "                break\n",
        "            elif key == 32:\n",
        "                t0 = time.time()\n",
        "                detect(model, processor, prompts=prompts, device=device, frame=frame)\n",
        "                t1 = time.time()\n",
        "                print(f\"Time for detection = {t1-t0}\")\n",
        "                cv2.waitKey()\n",
        "        # Break the loop\n",
        "        else:\n",
        "            break\n",
        "    # When everything done, release\n",
        "    # the video capture object\n",
        "    capture.release()\n",
        "\n",
        "    # Closes all the frames\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QAoys8GaDn-_"
      },
      "outputs": [],
      "source": [
        "# show_video_and_detect(\"/content/The Devil Wears Prada (45) Movie CLIP - Andy Gets a Makeover (2006) HD.mp4\", prompts=[\"black t-shirt\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FoM4XYOL4x2"
      },
      "source": [
        "##  Offline object detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-thgmO98L8_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b4b638-08f4-4262-c600-b6814de2bf4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pafy\n",
            "  Downloading pafy-0.5.5-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: pafy\n",
            "Successfully installed pafy-0.5.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pafy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BkfA5iTpMQZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f9db707-d005-425f-bd15-fcabc60c5794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube_dl==2020.12.7\n",
            "  Downloading youtube_dl-2020.12.7-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube_dl\n",
            "Successfully installed youtube_dl-2020.12.7\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube_dl==2020.12.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "o4oo-EHZMV27"
      },
      "outputs": [],
      "source": [
        "import pafy\n",
        "import random\n",
        "import youtube_dl\n",
        "import cv2 as cv\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_c-M_WGVf7Yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d459e1ad-258a-4cbd-e958-65b374eb5648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] HSPYgwP9R84: Downloading webpage\n",
            "{'format_id': '396', 'url': 'https://rr2---sn-5uaeznl6.googlevideo.com/videoplayback?expire=1682727202&ei=wgxMZNS5A4aa8wTW3aHgAQ&ip=35.196.134.206&id=o-AHnFPm-8xykeBZIJlNUpijCHisECDqqex61l-YDplDCB&itag=396&aitags=133%2C134%2C135%2C136%2C137%2C160%2C242%2C243%2C244%2C247%2C248%2C278%2C394%2C395%2C396%2C397%2C398%2C399&source=youtube&requiressl=yes&mh=6q&mm=31%2C26&mn=sn-5uaeznl6%2Csn-5go7yner&ms=au%2Conr&mv=u&mvi=2&pl=24&vprv=1&mime=video%2Fmp4&ns=00gV1_kmG2BmqD0oKi0afz4N&gir=yes&clen=3911216&dur=159.534&lmt=1617694467598556&mt=1682705344&fvip=5&keepalive=yes&fexp=24007246&c=WEB&txp=5531432&n=vuOvz8IFGO_-pZQ-3&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cvprv%2Cmime%2Cns%2Cgir%2Cclen%2Cdur%2Clmt&sig=AOq0QJ8wRQIhAPNnxZtrV32D5uMWWQ2bHW1P7XP70gPwXW20zWkfJFcTAiBOEVWv-r7k7ePFBeTP4RIBgZ0KmCHuWcjpkRcVI4MIhg%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl&lsig=AG3C_xAwRQIgauM-7N6BLy_u9sqcgvFcxQTH5N7dVIA96bxB448hnZwCIQC4U2CCP5x8Hn1m7fhbPG6KruyRctcOdOZOdtdLsHrRlw%3D%3D&ratebypass=yes', 'player_url': None, 'acodec': 'none', 'vcodec': 'av01.0.01M.08', 'asr': None, 'filesize': 3911216, 'format_note': '360p', 'fps': 24, 'height': 360, 'tbr': 293.189, 'width': 640, 'ext': 'mp4', 'downloader_options': {'http_chunk_size': 10485760}, 'format': '396 - 640x360 (360p)', 'protocol': 'https', 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.21 Safari/537.36', 'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'en-us,en;q=0.5'}}\n",
            "{'format_id': '134', 'url': 'https://rr2---sn-5uaeznl6.googlevideo.com/videoplayback?expire=1682727202&ei=wgxMZNS5A4aa8wTW3aHgAQ&ip=35.196.134.206&id=o-AHnFPm-8xykeBZIJlNUpijCHisECDqqex61l-YDplDCB&itag=134&aitags=133%2C134%2C135%2C136%2C137%2C160%2C242%2C243%2C244%2C247%2C248%2C278%2C394%2C395%2C396%2C397%2C398%2C399&source=youtube&requiressl=yes&mh=6q&mm=31%2C26&mn=sn-5uaeznl6%2Csn-5go7yner&ms=au%2Conr&mv=u&mvi=2&pl=24&vprv=1&mime=video%2Fmp4&ns=00gV1_kmG2BmqD0oKi0afz4N&gir=yes&clen=4020400&dur=159.534&lmt=1512882658926150&mt=1682705344&fvip=5&keepalive=yes&fexp=24007246&c=WEB&n=vuOvz8IFGO_-pZQ-3&sparams=expire%2Cei%2Cip%2Cid%2Caitags%2Csource%2Crequiressl%2Cvprv%2Cmime%2Cns%2Cgir%2Cclen%2Cdur%2Clmt&sig=AOq0QJ8wRQIhAPvJ5_vHkUEzKBqcFS6lzuPRJOVmLdGZ2dMATeDZ_ETVAiBkgNIQie5Pp5v-DVZt8_YlI0LoAFiVKl0hxOryDblRVA%3D%3D&lsparams=mh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl&lsig=AG3C_xAwRQIgauM-7N6BLy_u9sqcgvFcxQTH5N7dVIA96bxB448hnZwCIQC4U2CCP5x8Hn1m7fhbPG6KruyRctcOdOZOdtdLsHrRlw%3D%3D&ratebypass=yes', 'player_url': None, 'ext': 'mp4', 'height': 360, 'format_note': '360p', 'vcodec': 'avc1.4d401e', 'asr': None, 'filesize': 4020400, 'fps': 24, 'tbr': 346.202, 'width': 640, 'acodec': 'none', 'downloader_options': {'http_chunk_size': 10485760}, 'format': '134 - 640x360 (360p)', 'protocol': 'https', 'http_headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.21 Safari/537.36', 'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.7', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Encoding': 'gzip, deflate', 'Accept-Language': 'en-us,en;q=0.5'}}\n"
          ]
        }
      ],
      "source": [
        "# Get the videon \n",
        "ydl_opts = {}\n",
        "ydl=youtube_dl.YoutubeDL(ydl_opts)\n",
        "info_dict=ydl.extract_info(\"https://www.youtube.com/watch?v=HSPYgwP9R84\", download=False)\n",
        "formats = info_dict.get('formats', None)\n",
        "for f_number, f in enumerate(formats):\n",
        "  if f.get('format_note', None) == '360p' and f.get('ext', None) == 'mp4' and f.get('filesize', None) != None:\n",
        "    print(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to adjust the frame"
      ],
      "metadata": {
        "id": "8qL7Zov-RmxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_frame(frame, patch_size):\n",
        "  def reduce_size_black_rows(frame, patch_size, upper_limit, lower_limit):\n",
        "    \"\"\"\n",
        "    Performs in-place reduction of the size of the black rows\n",
        "    \"\"\"\n",
        "    zeros_row = np.zeros(frame.shape[1:]).astype(int)\n",
        "    indices = np.array([i for i in range(frame.shape[0]) if (frame[i,:,:] == zeros_row).all()])\n",
        "    if len(indices) > 0:\n",
        "      upper_indices = indices[indices <= lower_limit]\n",
        "      lower_indices = indices[indices >= upper_limit]\n",
        "      #print(f'The shape of the frame is {frame.shape}')\n",
        "      #print(f'The shape of upper_indices is {upper_indices.shape}')\n",
        "      #print(f'The shape of lower_indices is {lower_indices.shape}')\n",
        "      # a border = patch_size/2 is left at the top and the bottom of the frame\n",
        "      if upper_indices.shape[0] > 0:\n",
        "        min_row_idx = max([upper_indices[len(upper_indices)-1] - int(patch_size/2),0])\n",
        "      else:\n",
        "        min_row_idx = 0\n",
        "      if lower_indices.shape[0] > 0:\n",
        "        max_row_idx = min([lower_indices[0] + int(patch_size/2),frame.shape[0]])\n",
        "      else:\n",
        "        max_row_idx = frame.shape[0]\n",
        "      \n",
        "      rows_idxs = np.zeros(frame.shape[0]).astype(bool)\n",
        "      rows_idxs[min_row_idx:max_row_idx+1] = 1\n",
        "\n",
        "      return frame[rows_idxs,:,:]\n",
        "    return frame\n",
        "\n",
        "\n",
        "  def add_black_columns(frame, patch_size):\n",
        "    \"\"\"\n",
        "    Performs in-place black columns concatenation to frame\n",
        "    \"\"\"\n",
        "    n_columns = int(patch_size/2)\n",
        "    vertical_black_col = np.zeros((frame.shape[0],n_columns,3)).astype(int)\n",
        "    frame = np.concatenate((frame,vertical_black_col), axis=1)\n",
        "    frame = np.concatenate((vertical_black_col,frame), axis=1)\n",
        "    return frame\n",
        "\n",
        "\n",
        "  #frame = reduce_size_black_rows(frame, patch_size, upper_limit=430, lower_limit=50)\n",
        "  frame = add_black_columns(frame, patch_size)\n",
        "\n",
        "  return frame"
      ],
      "metadata": {
        "id": "wvp1SzfBOCN-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4nVJr78DLJ-l"
      },
      "outputs": [],
      "source": [
        "def detect_objects(video_url, prompts, frame_per_second=5, patch_size=64, window=3, stride=1, threshold=0.5, single_box=True, frames_union=None):\n",
        "  \"\"\"\n",
        "  Function to detect the specified object in each frame of the video\n",
        "  @param: video_url is the link to the video to analyse\n",
        "  @param: prompts is a list of texts descriving the objects to detect\n",
        "  @param: frame_per_second is the number of frame to analyse in a second\n",
        "  @param: it indicates the number of frames to concatenate in the same image\n",
        "  @param: the others are related to the object detection\n",
        "  \"\"\"\n",
        "  # Get the video\n",
        "  ydl_opts = {}\n",
        "  ydl=youtube_dl.YoutubeDL(ydl_opts)\n",
        "  info_dict=ydl.extract_info(video_url, download=False)\n",
        "\n",
        "  # Instanciate the lists\n",
        "  images_list = []\n",
        "  filenames_list = []\n",
        "  bounding_box_list = []\n",
        "\n",
        "  model, processor, device = load_model()\n",
        "\n",
        "  formats = info_dict.get('formats', None)\n",
        "  print(\"Obtaining frames\")\n",
        "  for f_number, f in enumerate(formats):\n",
        "\n",
        "    # If the resolution is 360p and if the format is webm (faster than mp4)\n",
        "    if f.get('format_note', None) == '360p' and f.get('ext', None) == 'webm' and f.get('filesize', None) != None:\n",
        "      # Get the url\n",
        "      url = f.get('url', None)\n",
        "\n",
        "      # Define how many frames to skip between each analysis\n",
        "      skip_frames = int(f['fps'] / frame_per_second)\n",
        "\n",
        "      cap = cv.VideoCapture(url)\n",
        "      current_frame = 0\n",
        "      t0 = time.time()\n",
        "      # Till the end of the video\n",
        "      while True:\n",
        "        print(f'Computing frame number {len(images_list)}')\n",
        "        # Get the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(f'Finished the video')\n",
        "            break\n",
        "\n",
        "        # Add columns to the frame to improve the quality of the detections\n",
        "        frame = adjust_frame(frame, patch_size)\n",
        "\n",
        "\n",
        "        # Define a name for the frame\n",
        "        filename = r\"video_shot\" + str(len(images_list)) + \".png\"\n",
        "        # If save the frame\n",
        "        # cv.imwrite(filename, frame)\n",
        "        # Append the frame and the filename in the lists\n",
        "        images_list.append(frame)\n",
        "        filenames_list.append(filename)\n",
        "        bounding_boxes = detect(model,\n",
        "                                processor, \n",
        "                                prompts=prompts, \n",
        "                                device=device, \n",
        "                                frame=frame,\n",
        "                                patch_size=patch_size, \n",
        "                                window=window, \n",
        "                                stride=stride, \n",
        "                                threshold=threshold,\n",
        "                                single_box=single_box)\n",
        "        bounding_box_list.append(bounding_boxes)\n",
        "\n",
        "        # Skip some frames\n",
        "        current_frame += skip_frames\n",
        "        cap.set(1, current_frame)\n",
        "        if cv.waitKey(30) & 0xFF == ord('q'):\n",
        "            break\n",
        "      print(\"Saved {} images with format {} and resolution {} in {:.4} seconds ({:.4} minutes)\".format(len(images_list), f.get('ext', None), f.get('format_note',None), (time.time() - t0), (time.time() - t0) / 60 ))\n",
        "      # If a valid format has been found and analysed\n",
        "      # if len(images_list) > 0:\n",
        "      #  break\n",
        "      # cap.release()\n",
        "  # Return the lists\n",
        "  return  images_list, filenames_list, bounding_box_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "n9l0IiIUDyCT"
      },
      "outputs": [],
      "source": [
        "def show_frames(images, row_dim=4, num_of_images=None):\n",
        "  \"\"\"\n",
        "  Function to show all the frames in the list\n",
        "  @params: images is the list of images\n",
        "  @params: row_dim is the number of images to show in a row\n",
        "  @param: num_of_images indicates how many images to show\n",
        "  \"\"\"\n",
        "  if num_of_images == None or num_of_images <= 0 or num_of_images > len(images):\n",
        "    num_of_images = len(images)\n",
        "\n",
        "  fig, axs = plt.subplots(int(num_of_images / row_dim) + 1, row_dim, figsize=(20, len(images[0][0]) // row_dim))\n",
        "\n",
        "  for i in tqdm(range(num_of_images)):\n",
        "    axs[int(i/row_dim), i%row_dim].imshow(images[i])\n",
        "    axs[int(i/row_dim), i%row_dim].set_title('Frame: {}'.format(i))\n",
        "\n",
        "  plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RASBg4raPnO_"
      },
      "outputs": [],
      "source": [
        "def show_frames_with_bounding_box(images_list, bounding_boxes, row_dim=6, num_of_images=None):\n",
        "  \"\"\"\n",
        "  Function to show all the frames in the list with the found bounding box\n",
        "  @params: images_list is the list of images\n",
        "  @params: bounding_boxes is a list of bounding box associated with the images.\n",
        "            For each image there a list of tuple: (x, y, width, heigth)\n",
        "  @params: row_dim is the number of images to show in a row\n",
        "  @param: num_of_images indicates how many images to show\n",
        "  \"\"\"\n",
        "  # Create a copy of the list\n",
        "  images = [image.astype(np.uint8) for image in images_list]\n",
        "\n",
        "  if num_of_images == None or num_of_images <= 0 or num_of_images > len(images):\n",
        "    num_of_images = len(images)\n",
        "\n",
        "  if int(num_of_images / row_dim) == 1:\n",
        "    fig, axs = plt.subplots(int(num_of_images / row_dim), row_dim, figsize=(20, len(images[0][0]) // row_dim))  \n",
        "    for i in tqdm(range(num_of_images)):\n",
        "      # If there is at least one box\n",
        "      if len(bounding_boxes[i]) > 0:\n",
        "        # img = np.array(images[i])\n",
        "        for bb in bounding_boxes[i]:\n",
        "          images[i] = cv2.rectangle(images[i], # frame\n",
        "                        (bb[0], # x\n",
        "                        bb[1]),# y \n",
        "                        (bb[0]+bb[2], # width\n",
        "                        bb[1]+bb[3]),# length \n",
        "                        (random.randint(128, 255), random.randint(128, 255), random.randint(128, 255))) # random color\n",
        "        axs[i%row_dim].imshow(images[i])\n",
        "        axs[i%row_dim].set_title('Frame: {}'.format(i))\n",
        "\n",
        "    plt.plot()\n",
        "    return\n",
        "\n",
        "\n",
        "  fig, axs = plt.subplots(int(np.ceil(num_of_images / row_dim)), row_dim, figsize=(20, len(images[0][0]) // row_dim))\n",
        "\n",
        "  for i in tqdm(range(num_of_images)):\n",
        "    # If there is at least one box\n",
        "    if len(bounding_boxes[i]) > 0:\n",
        "      # img = np.array(images[i])\n",
        "      for bb in bounding_boxes[i]:\n",
        "        images[i] = cv2.rectangle(images[i], # frame\n",
        "                      (bb[0], # x\n",
        "                       bb[1]),# y \n",
        "                      (bb[0]+bb[2], # width\n",
        "                       bb[1]+bb[3]),# length \n",
        "                      (random.randint(128, 255), random.randint(128, 255), random.randint(128, 255))) # random color\n",
        "    axs[int(i/row_dim), i%row_dim].imshow(images[i])\n",
        "    axs[int(i/row_dim), i%row_dim].set_title('Frame: {}'.format(i))\n",
        "\n",
        "  plt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0Q0Etpg9Qv3w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "580b9f9e-7cbd-40da-fab0-dd7dfdf97f56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'images_list, image_names_list, bounding_box_list = detect_objects(\\n    video_url=\"https://www.youtube.com/watch?v=HSPYgwP9R84\", # The devil wears Prada\\n    # prompts=[\"a lamp\", \"a black jacket\"],\\n    prompts=[\"a lamp\"],\\n    frame_per_second=0.1, \\n    patch_size=18, \\n    window=10,\\n    stride=1, \\n    threshold=0.2,\\n    single_box=False,\\n    frames_union=4)\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "'''images_list, image_names_list, bounding_box_list = detect_objects(\n",
        "    video_url=\"https://www.youtube.com/watch?v=HSPYgwP9R84\", # The devil wears Prada\n",
        "    # prompts=[\"a lamp\", \"a black jacket\"],\n",
        "    prompts=[\"a lamp\"],\n",
        "    frame_per_second=0.1, \n",
        "    patch_size=18, \n",
        "    window=10,\n",
        "    stride=1, \n",
        "    threshold=0.2,\n",
        "    single_box=False,\n",
        "    frames_union=4)\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HoipMwX0DC17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "47dd1a6e-e27e-4d77-ba83-2e30116cda34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'images_list, image_names_list, bounding_box_list = detect_objects(\\n    video_url=\"https://www.youtube.com/watch?v=NsRLOV4pHyk&t=6s\", # iron-man vs Loki\\n    prompts=[\"a green t-shirt\"],\\n    frame_per_second=0.1, \\n    patch_size=24, \\n    window=10,\\n    stride=1, \\n    threshold=0.1,\\n    single_box=False)\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "'''images_list, image_names_list, bounding_box_list = detect_objects(\n",
        "    video_url=\"https://www.youtube.com/watch?v=NsRLOV4pHyk&t=6s\", # iron-man vs Loki\n",
        "    prompts=[\"a green t-shirt\"],\n",
        "    frame_per_second=0.1, \n",
        "    patch_size=24, \n",
        "    window=10,\n",
        "    stride=1, \n",
        "    threshold=0.1,\n",
        "    single_box=False)\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nrytpoPHTpre"
      },
      "outputs": [],
      "source": [
        "#print(f'The number of frames is {len(images_list)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ADq8s5CwSJO-"
      },
      "outputs": [],
      "source": [
        "# show_frames(images=images_list, row_dim=8, num_of_images=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m7OE-7K4Xkt4"
      },
      "outputs": [],
      "source": [
        "# show_frames_with_bounding_box(images_list=images_list, bounding_boxes=bounding_box_list, row_dim=2, num_of_images=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try to concatenate frames before the detections to avoid useless bounding boxes"
      ],
      "metadata": {
        "id": "6bmvO422I9LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_frames(video_url, resolution='480p', frame_per_second=5, frames_union=None, patch_size=32):\n",
        "  \"\"\"\n",
        "  Function to detect the specified object in each frame of the video\n",
        "  @param: video_url is the link to the video to analyse\n",
        "  @param: resolution is the resolution of the video to acquire\n",
        "  @param: frame_per_second is the number of frame to analyse in a second\n",
        "  @param: frames_union it indicates the number of frames to concatenate in the same image\n",
        "  \"\"\"\n",
        "  # Get the video\n",
        "  ydl_opts = {}\n",
        "  ydl=youtube_dl.YoutubeDL(ydl_opts)\n",
        "  info_dict=ydl.extract_info(video_url, download=False)\n",
        "\n",
        "  # Instanciate the list\n",
        "  images_list = []\n",
        "\n",
        "  # Check the resolution\n",
        "  list_of_available_resolutions = ['240p', '360p', '480p', '720p']\n",
        "  if resolution not in list_of_available_resolutions:\n",
        "    resolution = list_of_available_resolutions[0]\n",
        "\n",
        "  formats = info_dict.get('formats', None)\n",
        "  print(\"Obtaining frames\")\n",
        "  for f_number, f in enumerate(formats):\n",
        "\n",
        "    # If the resolution is 360p and if the format is webm (faster than mp4)\n",
        "    if f.get('format_note', None) == resolution and f.get('ext', None) == 'webm' and f.get('filesize', None) != None:\n",
        "      # Get the url\n",
        "      url = f.get('url', None)\n",
        "\n",
        "      # Define how many frames to skip between each analysis\n",
        "      skip_frames = int(f['fps'] / frame_per_second)\n",
        "\n",
        "      cap = cv.VideoCapture(url)\n",
        "      current_frame = 0\n",
        "      t0 = time.time()\n",
        "      # Till the end of the video\n",
        "      while True:\n",
        "        print(f'Computing frame number {len(images_list)}')\n",
        "        # Get the frame\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(f'Finished the video')\n",
        "            break\n",
        "\n",
        "        # Add columns to the frame to improve the quality of the detections\n",
        "        frame = adjust_frame(frame, patch_size)\n",
        "\n",
        "        # Save the shape of the frame\n",
        "        frame_dimension = frame.shape\n",
        "\n",
        "        # If save the frame\n",
        "        # cv.imwrite(filename, frame)\n",
        "        # Append the frame and the filename in the lists\n",
        "        images_list.append(frame)\n",
        "\n",
        "        # Skip some frames\n",
        "        current_frame += skip_frames\n",
        "        cap.set(1, current_frame)\n",
        "        if cv.waitKey(30) & 0xFF == ord('q'):\n",
        "            break\n",
        "      print(\"Saved {} images with format {} and resolution {} in {:.4} seconds ({:.4} minutes)\".format(len(images_list), f.get('ext', None), f.get('format_note',None), (time.time() - t0), (time.time() - t0) / 60 ))\n",
        "      # If a valid format has been found and analysed\n",
        "      # if len(images_list) > 0:\n",
        "      #  break\n",
        "      # cap.release()\n",
        "\n",
        "  # Check if need to concatenate images, if not return the images\n",
        "  if frames_union == None or frames_union <= 0:\n",
        "    # Return the lists\n",
        "    return  images_list\n",
        "\n",
        "  # TODO: right now it's just a 2x2 concatenation: generalize it if it works\n",
        "  print(f'The number of frames is {len(images_list)}')\n",
        "\n",
        "  # If here, concatenate frames\n",
        "  # First, concatenate pairs of frames horizontally\n",
        "  horizontal_pairs = []\n",
        "  # Define a bool variable to know if the last images in the h pairs is alone\n",
        "  final_single_image = False\n",
        "\n",
        "  for i in range(0, len(images_list), 2):\n",
        "    # print(f'Computing horizontal aggregation number {i}')\n",
        "    # If there is a pair\n",
        "    if i+1 < len(images_list):\n",
        "      horizontal_pairs.append(cv2.hconcat([images_list[i], images_list[i+1]]))\n",
        "    elif i < len(images_list):\n",
        "      # Add the single image\n",
        "      horizontal_pairs.append(images_list[i])\n",
        "      # Save that the last image is single\n",
        "      final_single_image = True\n",
        "\n",
        "  print(f'The number of horizontal pairs is {len(horizontal_pairs)}')\n",
        "  \n",
        "  # Now let's concatenate the pairs vertically\n",
        "  vertical_pairs = []\n",
        "\n",
        "  for i in range(0, len(horizontal_pairs), 2):\n",
        "    # print(f'Computing vertical aggregation number {i}')\n",
        "    if not final_single_image:\n",
        "    # Append the pair if it's not the last one\n",
        "      if i+1 < len(horizontal_pairs):\n",
        "        vertical_pairs.append(cv2.vconcat([horizontal_pairs[i], horizontal_pairs[i+1]]))\n",
        "      elif i < len(horizontal_pairs):\n",
        "        vertical_pairs.append(horizontal_pairs[i])\n",
        "    else:\n",
        "      # If the number of images is even\n",
        "      if len(horizontal_pairs) % 2 == 0:\n",
        "        if i < len(horizontal_pairs)-2:\n",
        "          # Concatenate the images\n",
        "          vertical_pairs.append(cv2.vconcat([horizontal_pairs[i], horizontal_pairs[i+1]]))\n",
        "        else:\n",
        "          # Append the last 2 images as they are\n",
        "          vertical_pairs.append(horizontal_pairs[i])\n",
        "          vertical_pairs.append(horizontal_pairs[i+1])\n",
        "  \n",
        "  print(f'The number of vertical pairs is {len(vertical_pairs)}')\n",
        "\n",
        "  # Return the list\n",
        "  return vertical_pairs, frame_dimension"
      ],
      "metadata": {
        "id": "UUzmsnN3EMah"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scores(model, processor, device, prompts, frame, patch_size=64, window=3, stride=1):\n",
        "    \"\"\"\n",
        "    Function to the detect the objects in the frame. It uses the frames to look for the specified items.\n",
        "    It creates a plot of the image containing the detected objects.\n",
        "    :param model: model to run for the inference\n",
        "    :param processor: processor associated to the model\n",
        "    :param device: the hardware used to run the inference\n",
        "    :param prompts: the objects to find in the frame\n",
        "    :param frame: the specified frame\n",
        "    :param patch_size: the size of the patches\n",
        "    :param window: the amount of patches to search in simultaneously\n",
        "    :return: the scores of the detection\n",
        "    \"\"\"\n",
        "    colors = ['#FAFF00', '#8CF1FF']\n",
        "    # build image patches for detection\n",
        "    frame_patches = get_frame_patches(frame, patch_size)\n",
        "    frame_t = frame_to_tensor(frame)\n",
        "\n",
        "    scores_list = []\n",
        "\n",
        "    # process image through object detection steps\n",
        "    for i, prompt in enumerate(tqdm(prompts)):\n",
        "        scores = run_inference(model, processor, device, prompt, frame_patches, patch_size, window, stride)\n",
        "        scores_list.append(scores)\n",
        "        \n",
        "    return scores_list"
      ],
      "metadata": {
        "id": "P1uh9jt3j-Am"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aZuO4D1JJN2-"
      },
      "outputs": [],
      "source": [
        "def detect_objects_on_concatenate_images(images, prompts, patch_size=64, window=3, stride=1):\n",
        "  \"\"\"\n",
        "  Function to detect the specified object in each frame of the video\n",
        "  @param: images is a list of frames to analyse\n",
        "  @param: prompts is a list of texts descriving the objects to detect\n",
        "  @param: the others are related to the object detection\n",
        "  \"\"\"\n",
        "\n",
        "  scores_list = []\n",
        "\n",
        "  model, processor, device = load_model()\n",
        "\n",
        "  for frame in tqdm(images):\n",
        "        # If save the frame\n",
        "        # cv.imwrite(filename, frame)\n",
        "        scores = get_scores(model,\n",
        "                            processor, \n",
        "                            prompts=prompts, \n",
        "                            device=device, \n",
        "                            frame=frame,\n",
        "                            patch_size=patch_size, \n",
        "                            window=window, \n",
        "                            stride=stride)\n",
        "        scores_list.append(scores)\n",
        "\n",
        "  return  images, scores_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_concatenated_images(images, scores, frame_size):\n",
        "  \"\"\"\n",
        "  Given the list of concatenated images, divide it in single images according to\n",
        "  how the concatenation was done\n",
        "  TODO: now it's 4x4, update it in the future\n",
        "  @param: images is the list of concatenated images\n",
        "  @param: scores associated to the images\n",
        "  @param: frame_size is the dimension of a single frame \n",
        "  \"\"\"\n",
        "\n",
        "  single_images = []\n",
        "  single_scores = []\n",
        "\n",
        "  # For each concatenated image\n",
        "  for i in tqdm(range(len(images))):\n",
        "\n",
        "    # Convert the scores to a numpy array\n",
        "    score = np.array(scores[i][0])# 0 because just 1 prompt \n",
        "\n",
        "    # Check if the image is a 2x2, 2x1 or 1x1 concatenation\n",
        "    # If 2x2\n",
        "    if images[i].shape[0] > frame_size[0] and images[i].shape[1] > frame_size[1]:\n",
        "      single_images.append(images[i][0:frame_size[0], 0:frame_size[1]])\n",
        "      single_images.append(images[i][0:frame_size[0], frame_size[1]: ])\n",
        "      single_images.append(images[i][frame_size[0]: , 0:frame_size[1]])\n",
        "      single_images.append(images[i][frame_size[0]: , frame_size[1]: ])\n",
        "\n",
        "      single_scores.append(score[0:score.shape[0]//2, 0:score.shape[1]//2])\n",
        "      single_scores.append(score[0:score.shape[0]//2, score.shape[1]//2: ])\n",
        "      single_scores.append(score[score.shape[0]//2: , 0:score.shape[1]//2])\n",
        "      single_scores.append(score[score.shape[0]//2: , score.shape[1]//2: ])\n",
        "    # If 2x1\n",
        "    elif images[i].shape[1] > frame_size[1]:\n",
        "      single_images.append(images[i][:, 0:frame_size[1]])\n",
        "      single_images.append(images[i][:, 0:frame_size[1]]) \n",
        "\n",
        "      single_scores.append(score[:, 0:score.shape[1]//2])\n",
        "      single_scores.append(score[:, score.shape[1]//2:])  \n",
        "    # If 1x1\n",
        "    else:\n",
        "      single_images.append(images[i]) \n",
        "      single_scores.append(score)\n",
        "\n",
        "  return single_images, single_scores\n"
      ],
      "metadata": {
        "id": "Ivq_NxvelM1n"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bounding_boxes(scores_list, patch_size, threshold):\n",
        "  \"\"\"\n",
        "  For each score, search for bounding boxes\n",
        "  \"\"\"\n",
        "\n",
        "  bounding_boxes_list = []\n",
        "\n",
        "  for score in scores_list:\n",
        "    # Get all the bb in the frame\n",
        "    bb_list = get_multi_box(score, patch_size, threshold)\n",
        "    # Append all of them in the list\n",
        "    bounding_boxes_list.append(bb_list)\n",
        "\n",
        "  return bounding_boxes_list"
      ],
      "metadata": {
        "id": "hWJG3DpqsaTI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eaXIJS2KTSEn"
      },
      "outputs": [],
      "source": [
        "def save_frames_with_bounding_box(images_list, bounding_boxes):\n",
        "  \"\"\"\n",
        "  Function to all all the frames in the list with the found bounding box\n",
        "  @params: images_list is the list of images\n",
        "  @params: bounding_boxes is a list of bounding box associated with the images.\n",
        "            For each image there a list of tuple: (x, y, width, heigth)\n",
        "  \"\"\"\n",
        "  # Create a copy of the list\n",
        "  # images = [image.astype(np.uint8) for image in images_list]\n",
        "  images = images_list.copy()\n",
        "\n",
        "  for i in tqdm(range(len(images))):\n",
        "    # If there is at least one box\n",
        "    if len(bounding_boxes[i]) > 0:\n",
        "      # img = np.array(images[i])\n",
        "      for bb in bounding_boxes[i]:\n",
        "        images[i] = cv2.rectangle(images[i], # frame\n",
        "                      (bb[0], # x\n",
        "                      bb[1]),# y \n",
        "                      (bb[0]+bb[2], # width\n",
        "                      bb[1]+bb[3]),# length \n",
        "                      (random.randint(128, 255), random.randint(128, 255), random.randint(128, 255))) # random color\n",
        "\n",
        "    # Save the image\n",
        "    cv.imwrite(f'image_{i}.png', images[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the frames and concatenate them\n",
        "frames, frame_dimension = get_video_frames(video_url=\"https://www.youtube.com/watch?v=HSPYgwP9R84\", # The devil wears Prada\n",
        "                          resolution='360p', \n",
        "                          frame_per_second=0.1, \n",
        "                          frames_union=4)"
      ],
      "metadata": {
        "id": "9g9bJJxaJ5kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c71d85c-5fd0-4336-dfcd-c814cbcc5991"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] HSPYgwP9R84: Downloading webpage\n",
            "Obtaining frames\n",
            "Computing frame number 0\n",
            "Computing frame number 1\n",
            "Computing frame number 2\n",
            "Computing frame number 3\n",
            "Computing frame number 4\n",
            "Computing frame number 5\n",
            "Computing frame number 6\n",
            "Computing frame number 7\n",
            "Computing frame number 8\n",
            "Computing frame number 9\n",
            "Computing frame number 10\n",
            "Computing frame number 11\n",
            "Computing frame number 12\n",
            "Computing frame number 13\n",
            "Computing frame number 14\n",
            "Computing frame number 15\n",
            "Computing frame number 16\n",
            "Finished the video\n",
            "Saved 16 images with format webm and resolution 360p in 46.83 seconds (0.7805 minutes)\n",
            "The number of frames is 16\n",
            "The number of horizontal pairs is 8\n",
            "The number of vertical pairs is 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_frames(frames, row_dim=4, num_of_images=None)"
      ],
      "metadata": {
        "id": "RqkXm8QzGuZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the detections\n",
        "images_list, scores_detection_list = detect_objects_on_concatenate_images(images=frames,\n",
        "                                                                prompts=[\"a lamp\"],\n",
        "                                                                patch_size=18, \n",
        "                                                                window=10, \n",
        "                                                                stride=1)"
      ],
      "metadata": {
        "id": "6OvINjsZP25W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8c5acf-244e-4cfe-ead6-df85e360c390"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the patches = torch.Size([1, 40, 78, 3, 18, 18])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [01:07<00:00, 67.09s/it]\n",
            " 25%|██▌       | 1/4 [01:07<03:21, 67.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the patches = torch.Size([1, 40, 78, 3, 18, 18])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [01:04<00:00, 64.00s/it]\n",
            " 50%|█████     | 2/4 [02:11<02:10, 65.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the patches = torch.Size([1, 40, 78, 3, 18, 18])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [01:05<00:00, 65.02s/it]\n",
            " 75%|███████▌  | 3/4 [03:16<01:05, 65.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the patches = torch.Size([1, 40, 78, 3, 18, 18])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [01:06<00:00, 66.64s/it]\n",
            "100%|██████████| 4/4 [04:22<00:00, 65.71s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide the images and the scores\n",
        "single_images, scores_list = divide_concatenated_images(images=images_list,\n",
        "                                                        scores=scores_detection_list,\n",
        "                                                        frame_size=frame_dimension)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w81RZS_frAuo",
        "outputId": "c5e54698-f2ef-47aa-cb5e-2afa9f0a460d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 4576.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_frames(single_images, row_dim=4, num_of_images=None)"
      ],
      "metadata": {
        "id": "xukka1NbHz1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the bounding box\n",
        "bounding_boxes_list = get_bounding_boxes(scores_list=scores_list,\n",
        "                                         patch_size=18,\n",
        "                                         threshold=0.2)"
      ],
      "metadata": {
        "id": "YXCVz3qRr6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the bounding boxes\n",
        "show_frames_with_bounding_box(images_list=single_images,\n",
        "                              bounding_boxes=bounding_boxes_list,\n",
        "                              row_dim=4, \n",
        "                              num_of_images=None)"
      ],
      "metadata": {
        "id": "vAErS7Ywtqmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "save_frames_with_bounding_box(images_list=single_images,\n",
        "                              bounding_boxes=bounding_boxes_list)"
      ],
      "metadata": {
        "id": "vORFqpe7Ru7q",
        "outputId": "a1543c2c-ec0f-429f-849b-3e7354a7aee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 96.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract the sub-images from the frames"
      ],
      "metadata": {
        "id": "5AjIwIoEwg3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info_dict = {}\n",
        "\n",
        "# For each frame\n",
        "for frame_number in tqdm(range(len(single_images))):\n",
        "  # Instanciate the info_dict dictionary for the current frame\n",
        "  info_dict[frame_number] = {}\n",
        "  # For each bounding box associated to the current frame\n",
        "  for bb_index in range(len(bounding_boxes_list[frame_number])):\n",
        "    # Get the current bounding box\n",
        "    bb = bounding_boxes_list[frame_number][bb_index]\n",
        "    # Extract the object image from the frame\n",
        "    bb_image = single_images[frame_number][bb[1]: bb[1]+bb[3], bb[0]: bb[0]+bb[2]] \n",
        "    # Define a name for the sub-image\n",
        "    filename = r\"image_\" + str(frame_number) + \"_\" + str(bb_index) + \".png\"\n",
        "    # If save the sub-image if it is valid\n",
        "    if bb_image.shape[0] > 0 and bb_image.shape[1] > 0:\n",
        "      cv.imwrite(filename, bb_image)\n",
        "      # Save the details into the dictionary\n",
        "      info_dict[frame_number][bb_index] = {}\n",
        "      info_dict[frame_number][bb_index][\"Coordinates\"] = bb\n",
        "      info_dict[frame_number][bb_index][\"Image\"] = bb_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVJoVxH6wqYT",
        "outputId": "4f7033cb-2d29-4505-cc27-a2dcf89436ef"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 320.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of bounding boxes saved for each frame\n",
        "for frame_key, frame_item in info_dict.items():\n",
        "    print(f'Frame {frame_key} has {len(frame_item.keys())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HkB2Ja6zcSG",
        "outputId": "30f813b3-15b7-424f-c2bc-36ddfcb8042a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame 0 has 1\n",
            "Frame 1 has 2\n",
            "Frame 2 has 1\n",
            "Frame 3 has 2\n",
            "Frame 4 has 1\n",
            "Frame 5 has 1\n",
            "Frame 6 has 1\n",
            "Frame 7 has 0\n",
            "Frame 8 has 0\n",
            "Frame 9 has 1\n",
            "Frame 10 has 1\n",
            "Frame 11 has 2\n",
            "Frame 12 has 2\n",
            "Frame 13 has 1\n",
            "Frame 14 has 0\n",
            "Frame 15 has 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Siamese Network comparisons"
      ],
      "metadata": {
        "id": "mtuS24G89zZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Ds0hL35Ia6JS"
      },
      "outputs": [],
      "source": [
        "# Access the info_dict dictionary and compare each image saved"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1Kbo9bc-DgM"
      },
      "execution_count": 43,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tSh0U0s7C4Wm",
        "52wVwhfTMCKO"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}